import requests
from bs4 import BeautifulSoup
import time
import random
import pandas as pd
import re
import os


class LianJiaCrawler:
    def __init__(self):
        # ä½¿ç”¨ä½ ä»æµè§ˆå™¨è·å–çš„çœŸå®è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-CN,zh;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        # ä½¿ç”¨ä½ è·å–çš„URLä½œä¸ºåŸºç¡€
        self.base_url = "https://bj.lianjia.com/zufang/"

    def crawl_rental_list(self, page=1):
        """çˆ¬å–ç§Ÿæˆ¿åˆ—è¡¨é¡µ"""
        # æ„å»ºåˆ†é¡µURL
        if page == 1:
            url = self.base_url
        else:
            url = f"https://bj.lianjia.com/zufang/pg{page}/"

        print(f"ğŸ”„ æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ: {url}")

        try:
            # æ·»åŠ éšæœºå»¶æ—¶ï¼Œæ¨¡ä»¿äººç±»è¡Œä¸º
            time.sleep(random.uniform(2, 4))

            # å‘é€è¯·æ±‚
            response = requests.get(url, headers=self.headers, timeout=15)
            response.encoding = 'utf-8'  # è®¾ç½®ç¼–ç 

            # æ£€æŸ¥å“åº”çŠ¶æ€
            if response.status_code != 200:
                print(f"âŒ è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return None

            return response.text

        except Exception as e:
            print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
            return None

    def parse_rental_page(self, html_content, page=1):
        """è§£æç§Ÿæˆ¿é¡µé¢ï¼Œæå–æˆ¿æºä¿¡æ¯"""
        soup = BeautifulSoup(html_content, 'html.parser')

        # æ‰¾åˆ°æˆ¿æºåˆ—è¡¨å®¹å™¨
        content_list = soup.select_one('.content__list')
        if not content_list:
            print("âŒ æœªæ‰¾åˆ°æˆ¿æºåˆ—è¡¨å®¹å™¨")
            return []

        # æŸ¥æ‰¾æ‰€æœ‰æˆ¿æºé¡¹ç›®
        rental_items = content_list.select('.content__list--item')
        print(f"ğŸ“Š ç¬¬{page}é¡µæ‰¾åˆ° {len(rental_items)} ä¸ªæˆ¿æº")

        houses = []
        for i, item in enumerate(rental_items):
            try:
                house_info = self.parse_single_house(item)
                if house_info:
                    houses.append(house_info)
                    print(f"   âœ… è§£ææˆåŠŸ: {house_info['title'][:20]}... - {house_info['price']}å…ƒ/æœˆ")
            except Exception as e:
                print(f"   âŒ è§£æç¬¬{i + 1}ä¸ªæˆ¿æºå¤±è´¥: {e}")

        return houses

    def parse_single_house(self, item):
        """è§£æå•ä¸ªæˆ¿æºä¿¡æ¯"""
        house_info = {}

        # æå–æ ‡é¢˜
        title_elem = item.select_one('.content__list--item--title a')
        if title_elem:
            house_info['title'] = title_elem.text.strip()
            house_info['link'] = "https://bj.lianjia.com" + title_elem.get('href', '')

        # æå–ä»·æ ¼
        price_elem = item.select_one('.content__list--item-price em')
        if price_elem:
            price_text = price_elem.text.strip()
            # å¤„ç†ä»·æ ¼èŒƒå›´ï¼ˆå¦‚"2900-3250"ï¼‰
            if '-' in price_text:
                # å–ä»·æ ¼èŒƒå›´çš„å¹³å‡å€¼
                price_parts = price_text.split('-')
                try:
                    price1 = int(price_parts[0])
                    price2 = int(price_parts[1])
                    house_info['price'] = (price1 + price2) // 2  # å–å¹³å‡å€¼
                    house_info['price_range'] = price_text  # ä¿å­˜åŸå§‹ä»·æ ¼èŒƒå›´
                except:
                    house_info['price'] = 0
                    house_info['price_range'] = price_text
            else:
                try:
                    house_info['price'] = int(price_text)
                except:
                    house_info['price'] = 0

        # æå–æè¿°ä¿¡æ¯ï¼ˆåŒ…å«é¢ç§¯ã€æˆ·å‹ã€æ¥¼å±‚ç­‰ï¼‰
        desc_elem = item.select_one('.content__list--item--des')
        if desc_elem:
            desc_text = desc_elem.get_text(separator='|', strip=True)
            house_info['full_description'] = desc_text

            # ä»æè¿°ä¸­æå–å…·ä½“ä¿¡æ¯
            desc_parts = desc_text.split('|')
            for part in desc_parts:
                part = part.strip()
                # æå–é¢ç§¯
                if 'ã¡' in part:
                    house_info['area'] = part
                # æå–æˆ·å‹
                elif 'å®¤' in part and 'å…' in part:
                    house_info['layout'] = part
                # æå–æ¥¼å±‚
                elif 'å±‚' in part:
                    house_info['floor'] = part
                # æå–æœå‘
                elif 'ä¸œ' in part or 'å—' in part or 'è¥¿' in part or 'åŒ—' in part:
                    house_info['orientation'] = part
                # æå–ä½ç½®ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€ä¸ªéƒ¨åˆ†ï¼‰
                elif not any(key in part for key in ['ã¡', 'å®¤', 'å…', 'å±‚', 'ä¸œ', 'å—', 'è¥¿', 'åŒ—']):
                    if 'location' not in house_info:
                        house_info['location'] = part
                    else:
                        house_info['location'] += "|" + part

        # æå–å°åŒº/åŒºåŸŸ
        region_elem = item.select_one('.content__list--item--brand')
        if region_elem:
            house_info['region'] = region_elem.text.strip()

        # æå–æ ‡ç­¾ï¼ˆå¦‚ï¼šè¿‘åœ°é“ã€ç²¾è£…ç­‰ï¼‰
        tags_elems = item.select('.content__list--item--bottom oneline')
        if tags_elems:
            house_info['tags'] = [tag.text.strip() for tag in tags_elems]

        return house_info

    def crawl_multiple_pages(self, start_page=1, end_page=3):
        """çˆ¬å–å¤šé¡µæ•°æ®"""
        all_houses = []

        for page in range(start_page, end_page + 1):
            print(f"\n{'=' * 50}")
            print(f"å¤„ç†ç¬¬ {page} é¡µ")
            print(f"{'=' * 50}")

            html_content = self.crawl_rental_list(page)
            if not html_content:
                print(f"âŒ ç¬¬{page}é¡µçˆ¬å–å¤±è´¥ï¼Œè·³è¿‡")
                continue

            houses = self.parse_rental_page(html_content, page)
            all_houses.extend(houses)

            print(f"âœ… ç¬¬{page}é¡µå®Œæˆï¼Œè·å– {len(houses)} ä¸ªæˆ¿æº")

        print(f"\nğŸ‰ æ‰€æœ‰é¡µé¢å®Œæˆï¼å…±è·å– {len(all_houses)} ä¸ªæˆ¿æº")
        return all_houses

    def save_to_csv(self, houses, filename="lianjia_rentals.csv"):
        """ä¿å­˜æˆ¿æºä¿¡æ¯åˆ°CSVæ–‡ä»¶"""
        if not houses:
            print("âŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨ - ä¿®æ­£è·¯å¾„
        os.makedirs('../../output', exist_ok=True)

        df = pd.DataFrame(houses)
        filepath = f'../../output/{filename}'  # ä¿®æ­£è·¯å¾„
        df.to_csv(filepath, index=False, encoding='utf_8_sig')
        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")

        # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
        print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"   æˆ¿æºæ•°é‡: {len(houses)}")
        if 'price' in df.columns:
            print(f"   å¹³å‡ä»·æ ¼: {df['price'].mean():.0f}å…ƒ/æœˆ")
            print(f"   ä»·æ ¼èŒƒå›´: {df['price'].min()} - {df['price'].max()}å…ƒ/æœˆ")
        if 'area' in df.columns:
            print(f"   é¢ç§¯èŒƒå›´: {df['area'].unique()[:5]}...")  # æ˜¾ç¤ºå‰5ä¸ªä¸åŒçš„é¢ç§¯


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    crawler = LianJiaCrawler()

    print("=" * 60)
    print("           é“¾å®¶ç§Ÿæˆ¿çˆ¬è™« - å®Œæ•´æ•°æ®æå–")
    print("=" * 60)

    # çˆ¬å–å¤šé¡µæ•°æ®
    houses = crawler.crawl_multiple_pages(1, 3)

    if houses:
        # ä¿å­˜æ•°æ®
        crawler.save_to_csv(houses)

        # æ˜¾ç¤ºå‰3ä¸ªæˆ¿æºè¯¦æƒ…
        print(f"\nğŸ  å‰3ä¸ªæˆ¿æºè¯¦æƒ…:")
        for i, house in enumerate(houses[:3]):
            print(f"\næˆ¿æº {i + 1}:")
            for key, value in house.items():
                print(f"  {key}: {value}")
    else:
        print("âŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•æˆ¿æºæ•°æ®")